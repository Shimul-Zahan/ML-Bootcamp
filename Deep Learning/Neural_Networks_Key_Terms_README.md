
# Neural Networks - Key Terms

## Basic Concepts
- **Neuron** 🧩: A single unit in a neural network that processes inputs and produces an output.  
- **Layer** 🏗️: A collection of neurons that process data together.  
- **Input Layer** 🎯: The first layer that receives raw data.  
- **Hidden Layer(s)** 🔄: Intermediate layers where computations happen.  
- **Output Layer** 🎯: The final layer that gives predictions.  

## Parameters in NNs
- **Weights (W)** ⚖️: The adjustable parameters that determine the strength of connections.  
- **Bias (b)** ➕: A constant added to shift activations.  

## Mathematical Operations
- **Activation Function** 🔥: Defines how a neuron fires (e.g., ReLU, Sigmoid, Tanh).  
- **Forward Propagation** ➡️: The process of passing input through the network to get an output.  
- **Loss Function** 🎯: A function that measures the error (e.g., MSE, Cross-Entropy).  
- **Backpropagation** 🔄: The algorithm for updating weights based on the loss.  
- **Gradient Descent** 📉: Optimization algorithm that minimizes loss by updating weights.  

## Advanced Concepts
- **Learning Rate (α)** 🚀: Controls how much weights update in training.  
- **Overfitting** 🎭: When a model memorizes instead of generalizing.  
- **Underfitting** 📉: When a model is too simple to learn patterns.  
- **Dropout** 🎲: A technique to prevent overfitting by randomly deactivating neurons.  

## Specialized Types
- **CNN (Convolutional Neural Network)** 🖼️: For image processing.  
- **RNN (Recurrent Neural Network)** 🔄: For sequential data (e.g., time series, NLP).  
- **LSTM (Long Short-Term Memory)** 🧠: A type of RNN that handles long-term dependencies.  
- **Transformer** ⚡: Used in modern NLP (e.g., BERT, GPT).  
