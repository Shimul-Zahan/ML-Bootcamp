
# Neural Networks - Key Terms

## Basic Concepts
- **Neuron** ğŸ§©: A single unit in a neural network that processes inputs and produces an output.  
- **Layer** ğŸ—ï¸: A collection of neurons that process data together.  
- **Input Layer** ğŸ¯: The first layer that receives raw data.  
- **Hidden Layer(s)** ğŸ”„: Intermediate layers where computations happen.  
- **Output Layer** ğŸ¯: The final layer that gives predictions.  

## Parameters in NNs
- **Weights (W)** âš–ï¸: The adjustable parameters that determine the strength of connections.  
- **Bias (b)** â•: A constant added to shift activations.  

## Mathematical Operations
- **Activation Function** ğŸ”¥: Defines how a neuron fires (e.g., ReLU, Sigmoid, Tanh).  
- **Forward Propagation** â¡ï¸: The process of passing input through the network to get an output.  
- **Loss Function** ğŸ¯: A function that measures the error (e.g., MSE, Cross-Entropy).  
- **Backpropagation** ğŸ”„: The algorithm for updating weights based on the loss.  
- **Gradient Descent** ğŸ“‰: Optimization algorithm that minimizes loss by updating weights.  

## Advanced Concepts
- **Learning Rate (Î±)** ğŸš€: Controls how much weights update in training.  
- **Overfitting** ğŸ­: When a model memorizes instead of generalizing.  
- **Underfitting** ğŸ“‰: When a model is too simple to learn patterns.  
- **Dropout** ğŸ²: A technique to prevent overfitting by randomly deactivating neurons.  

## Specialized Types
- **CNN (Convolutional Neural Network)** ğŸ–¼ï¸: For image processing.  
- **RNN (Recurrent Neural Network)** ğŸ”„: For sequential data (e.g., time series, NLP).  
- **LSTM (Long Short-Term Memory)** ğŸ§ : A type of RNN that handles long-term dependencies.  
- **Transformer** âš¡: Used in modern NLP (e.g., BERT, GPT).  
