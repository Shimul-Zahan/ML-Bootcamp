{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the word and its parts like: Noun, Verb, Adj etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pos_tag = \"Taj Mahal is a beautiful Munument\"\n",
    "paragraph = \"Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables systems to learn from data and improve their performance over time without being explicitly programmed. It involves algorithms and statistical models that analyze patterns within data to make predictions or decisions. ML is used in various fields, such as healthcare for diagnosing diseases, finance for fraud detection, and e-commerce for personalized recommendations. With advancements in deep learning, a subset of ML, the capabilities of AI systems have grown significantly, allowing them to handle complex tasks like image recognition, natural language processing, and autonomous driving.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sentences = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('machine', 'NN'), ('learning', 'NN'), ('(', '('), ('ml', 'NN'), (')', ')'), ('subset', 'VBD'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('ai', 'NN'), (')', ')'), ('enables', 'VBZ'), ('systems', 'NNS'), ('learn', 'JJ'), ('data', 'NNS'), ('improve', 'VB'), ('performance', 'NN'), ('time', 'NN'), ('without', 'IN'), ('explicitly', 'RB'), ('programmed', 'VBN'), ('.', '.')]\n",
      "[('involves', 'NNS'), ('algorithms', 'VBP'), ('statistical', 'JJ'), ('models', 'NNS'), ('analyze', 'VBP'), ('patterns', 'NNS'), ('within', 'IN'), ('data', 'NNS'), ('make', 'VBP'), ('predictions', 'NNS'), ('decisions', 'NNS'), ('.', '.')]\n",
      "[('ml', 'NNS'), ('used', 'VBN'), ('various', 'JJ'), ('fields', 'NNS'), (',', ','), ('healthcare', 'NN'), ('diagnosing', 'VBG'), ('diseases', 'NNS'), (',', ','), ('finance', 'NN'), ('fraud', 'NN'), ('detection', 'NN'), (',', ','), ('e-commerce', 'JJ'), ('personalized', 'JJ'), ('recommendations', 'NNS'), ('.', '.')]\n",
      "[('advancements', 'NNS'), ('deep', 'JJ'), ('learning', 'NN'), (',', ','), ('subset', 'VBN'), ('ml', 'NN'), (',', ','), ('capabilities', 'VBZ'), ('ai', 'VBP'), ('systems', 'NNS'), ('grown', 'VBN'), ('significantly', 'RB'), (',', ','), ('allowing', 'VBG'), ('handle', 'NN'), ('complex', 'JJ'), ('tasks', 'NNS'), ('like', 'IN'), ('image', 'NN'), ('recognition', 'NN'), (',', ','), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), (',', ','), ('autonomous', 'JJ'), ('driving', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# find all the pos tag\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed_sentenced = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    # sentence = \" \".join(words)\n",
    "    # processed_sentenced.append(sentence)\n",
    "    \n",
    "    poss_tag = nltk.pos_tag(words)\n",
    "    print(poss_tag)\n",
    "# processed_sentenced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('monument', 'NN'), ('.', '.')], [('It', 'PRP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('seven', 'CD'), ('wonders', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Taj Mahal is a beautiful monument. It is one of the seven wonders of the world.\"\n",
    "\n",
    "# Step 1: Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Step 2: Word Tokenization for each sentence\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Step 3: POS Tagging for each sentence\n",
    "pos_tags = nltk.pos_tag_sents(tokenized_sentences)\n",
    "\n",
    "# Output\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taj', 'NNP'),\n",
       " ('Mahal', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('monument.', 'NN'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('seven', 'CD'),\n",
       " ('wonders', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world.', 'NN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
